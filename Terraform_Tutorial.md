## Introduction 
* Developed by **Hashi Corp** is used basically for Infrastructure as a code (IaaS)
* It is **better** than its competitive tools as it gives:
1) High level infra description 
2) Parallel management of resources
3) Separate planning from execution. 
* Provides a **coding workflow** for creation of infrastructure. 
* Can integrate with application code workflow
* **Configuration** is Human Readable as its developed in its own DSL called as **HCL(Hashi Corp Language)**.

It will be easy to understand, now lets look at the basic workflow that what things will workout in this tutorial

### Basic Overall Workflow [For now!]
* **First**, install all the softwares enlisted in prerequired lists.
* **Second**, prepare machine that will run the Terraform 
* **Third** prepare some required GCP infrastructure. 
* **Fourth**, write the terraform config that will be used to call the deployment module
* **Finally**, apply the config to deploy the cluster

Terraform Initialization **=>** Terraform Planning **=>** Terraform Apply **=>** (After use) **=>** Terraform Destroy

## Terraform Installation
* Download the binary file for your respective OS from the official terraform [site](https://www.terraform.io/downloads.html)
* Copy the binary to C/terraform directory in **windows** and to any directory in linux **linux**
* Give **Path** environment variable the path of terraform directory containing the binary. For **windows** & **Linux** \
For Linux you can write export PATH = "$PATH:~/DIRECTORY"
* Reassure your installation by writing **"Terraform --version"** in your cmd.

## Resources 
Theses are pretty much the infrastructre which we are going to provision on the Cloud platform. For that first we need the **plugins** for the **providers**(google or azure or aws) so that we can use the resources types through these plugins. \
Resources have a basic syntax in which theres is **resource type** and **local resource name** which will be used to access the resource on the local machine within the module. This name is just to access the resource within the module , it woudnt hold any importance outside the scope of the module in which it is defined.Using both resource type and resource name we will uniquely identify the object inside the script, so unique name should be given.
There will also be another name for the resource which we will define inside its block, this name will be projected on the platform for your particular resource.

**Basic Syntax** of Resource: 
```HCL 
resource ''resource_type'' ''resource_local_name"
{
//Configuration_arguments will come here for this resource
}
```

## Setup
* First we create an **Account** on GCP which is must required
* Second we create a **Project** in GCP to which we will provision resources using terraform
* To connect with the account project we will need a key which will be generated by creating a **Service Account**. 
* Now, In your local machine, create a new directory and name it as terraform scripts
* Terraform has its own DSL and its scripts have **".tf"** format. Create a script file with name **"main.tf"**
* Now in this script file we will writecode for linking this script to gcp account along with we will also write all the resources which we need to provision on the platform

## Linking local script with Account on GCP 
Now open main.tf and write the below code
```HCL
provider "google" {
  version = "3.5.0"

  credentials = file("Credentials.json")

  project = "<PROJECT_ID>"
  region  = "us-central1"
  zone    = "us-central1-c"
}
resource <RESOURCE TYPE> <RESOURCE NAME>
{
 //Arguments to specify the configurations of that particular resource
}
```

* As we created a service account in previous step we got a key with it which we downlaoded on our local machine, now rename that file with **Credentials.json** and put the file in the same directory in which this script resides. 
* **Project id** will be used to go to specific project of your GCP account(as you may have multiple projects), on the home page of GCP console you will get the project ID.
* You can specify the region and zone as per your choice in project.(You must know what is Region and Zone in GCP, if you have completed the previous tutorial on GCP.
* **Provider** is used to specify the platform which we are choosing like whether we are provisioning on google, or azure or aws etc. There can be multiple providers in a single script if we want the resources to be provisioned across various platforms.
* **Resource** is used to actually tell the script what **type** of resource we are going to provision and what will be its **name** and we configure it by writing all the properties.

Now after this save the file and come out to cmd and write the command **Terraform init** in your cmd, this will download all the **provider plugins** necessary for the script.
Output will look like this:
```HCL
Initializing the backend...

Initializing provider plugins...

The following providers do not have any version constraints in configuration,
so the latest version was installed.

To prevent automatic upgrades to new major versions that may contain breaking
changes, it is recommended to add version = "..." constraints to the
corresponding provider blocks in configuration, with the constraint strings
suggested below.

* provider.google: version = "~> 3.20"

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.
```
## Creating a resource using script on GCP
Now edit the main.tf file again and write the code for the resource which you want to create, lets say for now we want to create a compute network then the code for it will be:

```HCL
provider "google" {
  version = "3.5.0"

  credentials = file("Credentials.json")

  project = "<PROJECT_ID>"
  region  = "us-central1"
  zone    = "us-central1-c"
}

resource "google_compute_network" "vpc_network" {
  name = "terraform-network"
}
```

Now after this just like we compile our code in programming language you can also check this script out using terraform by writing the command in cmd **"Terraform Plan"**
Output will be like this:
```HCL
$ terraform plan

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # google_compute_network.vpc_network will be created
  + resource "google_compute_network" "vpc_network" {
      + auto_create_subnetworks         = true
      + delete_default_routes_on_create = false
      + gateway_ipv4                    = (known after apply)
      + id                              = (known after apply)
      + name                            = "terraform-network"
      + project                         = (known after apply)
      + routing_mode                    = (known after apply)
      + self_link                       = (known after apply)
    }

Plan: 1 to add, 0 to change, 0 to destroy.
```
* Now When you are confirm about the resources which are going to be created as said in the previous output type the next command which will start the provisioning of resources on GCP i.e. **Terraform Apply** command.
Output will be looking like below:
```HCL
Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

  Enter a value: yes
  
google_compute_instance.vm_instance: Creating...
google_compute_instance.vm_instance: Still creating... [10s elapsed]
google_compute_instance.vm_instance: Creation complete after 17s 
```
And Now you can go to your google cloud console and check inside the corresponding project for the resources provisioned through the script.
## Destroying the resources provisioned
Just after you are done with work on your resources then you can simply delete them with single command i.e. **Terraform destroy**

## Terraform Show command
If you dont want the long task of going to browser and then google cloud console then you can simply watch the active resources using the command **Terraform Show** and output will look like:

```HCL 
resource "google_compute_network" "vpc_network" {
    auto_create_subnetworks         = true
    delete_default_routes_on_create = false
    id                              = "terraform-network"
    name                            = "terraform-network"
    project                         = "hc-training-test"
    routing_mode                    = "REGIONAL
}
```

## Making Changes to Infrastructure
You can also add new resources to your script file anytime you want and hit the apply command for provisioning them. \
Sometimes you might just want to update some arguments of a resource like changing tag or names. In that case for example we add tags to  a compute instance resource in our script file: 
```HCL 
resource "google_compute_instance" "vm_instance" {
  name         = "terraform-instance"
  machine_type = "f1-micro"
  tags         = ["web", "dev"]
  # ...
}
```
Now when you write the command "Terraform plan" it will understand that it doesnt need to create the entire resource once again **rather make some minute changes** in it and the output would look like:
```HCL
 terraform apply
google_compute_network.vpc_network: Refreshing state... [id=terraform-network]
google_compute_instance.vm_instance: Refreshing state... [id=terraform-instance]

...

Terraform will perform the following actions:

  # google_compute_instance.vm_instance will be updated in-place
  ~ resource "google_compute_instance" "vm_instance" {

...

~ tags                 = [
    + "dev",
    + "web",
  ]

...

Plan: 0 to add, 1 to change, 0 to destroy.
```
Now just write the command "Terraform apply to finally save your changes and provisin the update version of resource.

## Resource Dependencies 
Sometimes when we have so much of resources available on the cloud it might happen that one resource will be used by other or we have to link two resources, or one resources need another in order to work correctly.
For example lets say the instance we created above needs to have a static IP address. Now for that we will have to create first a new resource which is a **"Google compute address"** resource and to create it add the following code to your **main.tf** script file

```HCL
resource "google_compute_address" "vm_static_ip" {
  name = "terraform-static-ip"
}
```
Now it was never discussed that how to access this particular resource elsewhere inside the script for that we use 
"Resource_type.Resourcename.property" method i.e. using dots we reach the particular resource or its property.

So coming back to adding IP address to our instance, now we shall assign IP created to instance by adding following code to instance resource.

```HCL
  network_interface {
    network = google_compute_network.vpc_network.self_link
    access_config {
      nat_ip = google_compute_address.vm_static_ip.address
    }
  }

```
For this terraform will ensure that first IP is created then the changes are saved and then Instance is created in order to assign it an IP address. So in this way terraform take cares of the order of creation of resources based upon the dependency. This is an example of **Implicit Dependency** as through the last line in the code terraform got to know that Instance depends upon "Google Compute address" so  IP needs to be created first.

If we want some particular resource to be created before another resource we can use the **depends_on** attribute under any resource suppose if we want for any application to run on an instance, we want a storage bucket to be created before the instance is created. This is the case of **Explicit Dependency** when we specify the order to terraform in which the resources should be created. As terraform sometimes cant guess the dependency if its not written in the code or we can say when its not **visible to terraform**.

Code for creating a bucket and then ading depends_on attribute to instance resource:
```HCL
resource "google_storage_bucket" "example_bucket" {
  name     = "<UNIQUE-BUCKET-NAME>"
  location = "US"
}
 resource "google_compute_instance" "another_instance" {
  # Tells Terraform that this VM instance must be created only after the
  # storage bucket has been created.
  depends_on = [google_storage_bucket.example_bucket] <--------------------------

  name         = "terraform-instance-2"
  machine_type = "f1-micro"

  boot_disk {
    initialize_params {
      image = "cos-cloud/cos-stable"
    }
  }

  network_interface {
    network = google_compute_network.vpc_network.self_link
    access_config {
    }
  }
}
```
This code doesn't need to be added to main.tf script as this was just an example code

## Terraform Provisioners
At times when we provision our environment, we want our OS images to be preconfigured with tools which are not preinstalled. For this purpose terraform provisioners were created which install tools or run scripts that do the preconfig stuff or execute some commands on local machine when the resource is created.

For example a terraform provisioner **local-exec** is used to run commands **when a new resource is created**. You just to add the provisioner along with what needs to be done inside the first instance resource block described in following way:
```HCL 
resource "google_compute_instance" "vm_instance" {
  name         = "terraform-instance"
  machine_type = "f1-micro"
  tags         = ["web", "dev"]

  provisioner "local-exec" {   
    command = "echo ${google_compute_instance.vm_instance.name}:  ${google_compute_instance.vm_instance.network_interface[0].access_config[0].nat_ip} >> ip_address.txt"
  }

  # ...
}

```
In this we use dot provider to go to Static IP given to the instance. So these commands will get executed whenever this instance will be **created**. Right now, we have already created this instance so even if we hit **terraform apply** because provisioners are designed to work only when the resource is being created so that the commands dont get executed everytime we make a chane in resource attributes.
So for destroying and recreating the resource we have another command in terraform called **terraform taint <resource_type>.<resource_name>**  So we write the following code 
```HCL
terraform taint google_compute_instance.vm_instance
```
and output will be :
```HCL
Resource instance google_compute_instance.vm_instance has been marked as tainted.
```
Now when we hit the **apply** command then the mentioned tainted resource will be destroyed and recreated. And this will result in execution of provisioners we created. 

**Note:** This was just one type of provisioner we learned which justs executes some command locally on our machine. There are other type of provisioners too through which we can execute scripts.

### Failed Provisioner and Tainted Resource
If in case while creating a resource the provisioner fails to execute then that resource will be **still marked "tainted"** and will be considered **not safe to use** as the provisioner failed.

### Destroy time Provisioners 
Just like these provisioners we can have provisioners which run while the resource is being destroyed lets say for system cleanup and making a last log note. Then we use these type of provisioners. It works just like a destructor in OOPS concept!.

## Input and Output Variables

### Input Variables
These can be used to remove hardcoded values into variables inside our script so that we dont have to everytime write the full id or value inside them, instead we can define them at one file place and reference them wherever we want in the same directory. \
First create a **"variables.tf"** file.
```HCL
variable "region" {
  default = "us-central1"
}

variable "zone" {
  default = "us-central1-c"
}
```
Terraform loads all the file in the directory with **".tf"** format, so it isn't necessary where your variables are defined which we discussed just above.

Now, we use the variables in our **main.tf** config file.
```HCL
provider "google" {
  version = "3.5.0"

  credentials = file(credentials_file.json)

  project = PROJECT_ID
  region  = var.region
  zone    = var.zone
}

```
**Note**: We reference each variable through **var.** keyword. 

Note: We can also make Project ID and Credentials file as a variable but that would mean that all scripts for provisioning infrastructure will be connected to same project id wherever we use the project variable.

### Assigning Variables
You can assign variable through mutliple ways
1) Through Command line in the form of **-var**. Any command of terraform which inspects the config file like plan, apply or refresh will work with this attribute.
```HCL
terraform plan -var 'project=<PROJECT_ID>'
```
Will only work one time only 

2) Through a seperate file named **"terraform.tfvars""**. Put the some contents inside the file like
```
project = "<PROJECT_ID>"
credentials_file = "<NAME>.json"
zone= us-central-1a
region = us-central 
```
Now whenever you use these variables in the same directory these will be easy referred as terraform automatically loads file with **.tfvars** format within the same folder or directory. \
These variables will be persisting as long as the file exists in the directory 

3) Through environment variables where you can specify the variables as terraform will read those environment variables with **TF_VAR_NAME** format. 

### Variable Types
Now lets discuss about various variable types as it can be a number or a string etc. 
#### Strings
By default if you dont assign any variable type then that variable will be of **String** type.
Also you can explicitly define a variable as string.
```HCL
variable "My_new_variable" {
  type = string <----- 
}
```
#### Numbers
Any **valid integer** or **floating** point will be called as a number. 
```HCL
variable "counter_instance" {
  type    = number
  default = 1
}
```
Default keyword is used for assigning default value of variable.

#### Maps
Maps are the way we create data structures in our programming language, you might want to assign a particular value to a variable for a particular index (index need not to be a number only). for example we say we need the value of machine type variable for different envrioments(indexes).
So we can configure our **Variables.tf** file in the way:
```HCL
variable "machine_type"
{
type="map"
default = {
            dev  = "f1-micro"
            test = "n1-highcpu-32"
             prod = "n1-highcpu-32"
            }
}
```

### Output Variables
These are used to organize data and show it back to terraform user. As there are multiple of attribute values of all our resources but we dont want to know them all when we run the script. So these output variables are used to list some of them which are more important like VPN,IP address , load balancer etc. These are displayed whenever we hit the **apply** command. 

For this purpose make a **"output.tf"** file and store the values of the variables in this file using the following syntax. And terraform will load all the **.tf** files whenever we hit the apply command.
```HCL

output "ip"
{
value= google_compute_address.vm_static_ip.address
}
```
**value** is used to store the value of attribute **ip**. 
So we can use **terraform refresh** command here to reconcile the difference between the local state file and real infrastructure on cloud. This does not change the infrastructure but surely does change the local state file.  
Output will look like this:

```HCL
google_compute_network.vpc_network: Refreshing state...  
google_compute_address.vm_static_ip: Refreshing state... 
google_compute_instance.vm_instance: Refreshing state...  

Outputs:

ip = 35.192.68.38
```

Okay so now we have learned much about terraform and how it works, Now lets practise some of the terraform scripts to implement what we learned.

## Practise Scripts
### Script @1
**Aim**: To provision Simple Compute Instance with detail arguments on GCP
Script is below:
```HCL
// Code for Connecting to account and its project 
provider "google" {
  version = "3.5.0"

  credentials = file("Credentials.json") //Here you give the key 

  project = "<PROJECT_ID>"
  region  = "us-central1"
  zone    = "us-central1-c"
}

// Code for Resource Instance

resource "google_compute_instance" "newinstance" {
  name         = "practisescriptoneinstance"
  machine_type = "f1-micro" //With lowest memory as we are just practising
  zone         = "us-central1-a"

boot_disk {
    initialize_params {
      image = "debian-cloud/debian-9" //OS image it will boot 
    }
  }
  
  network_interface {
    network = "default"

    access_config {
      // Ephemeral IP
    }
  }
  
  metadata_startup_script = "echo hi > /test.txt"
}
```
Now give your **Project id** and **Credentials file** and just hit the **terraform apply** command.

### Script @2
**Aim**: To provision Virtual Privated Cloud(VPC) with subnets =on GCP
Script is below:
```HCL
// Plugin or provider for accessing the resources on Google Cloud platform
provider "google" {
 credentials = file("credentials.json")
 project     = "${var.gcp_project}" 
 region      = "${var.region}"
}

// Create VPC and can be found on terraform google provider documentation
resource "google_compute_network" "vpc" {
 name                    = "${var.name}-vpc"
 auto_create_subnetworks = "false"
}

// Create Subnet and default syntax of resource can be taken from terraform docs
resource "google_compute_subnetwork" "subnet" {
 name          = "${var.name}-subnet"
 ip_cidr_range = "${var.subnet_cidr}"
 network       = "${var.name}-vpc"
 depends_on    = [
  	google_compute_network.vpc,
]
region      = "${var.region}"
}

// VPC firewall configuration
resource "google_compute_firewall" "firewall" {
  name    = "${var.name}-firewall"
  network = "${google_compute_network.vpc.name}"

  allow {
    protocol = "icmp"
  }

  allow {
    protocol = "tcp"
    ports    = ["22"]
  }

  source_ranges = ["0.0.0.0/0"]
}
```

So here we create a **vpc** along with one **subnetwork**. Also we create a **firewall** along with the network. 


For more Good and better scripts  I have started working on **Terraform Official CLI** documentation as i have completed basic tutorial from mutliple resources as of youtube, terraform documentation, blogs.

Terraform CLI link: - [Terraform CLI](https://www.terraform.io/docs/cli-index.html)











